{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "train_features = pd.read_csv(open(\"train_features.tsv\"),sep='\\t')\n",
    "train_labels = pd.read_csv(open(\"train_labels.tsv\"),sep='\\t')\n",
    "\n",
    "\n",
    "tags = np.array(train_features['tag'],dtype = np.str)\n",
    "list1 = []\n",
    "for i in range(tags.size):\n",
    "    list1.extend(tags[i].split(','))\n",
    "list2 = list(set(list1))\n",
    "list2.sort()\n",
    "tags1 = np.zeros((5240,200))\n",
    "for i in range(5240):\n",
    "    for j in range(200):\n",
    "        tempList = tags[i].split(',')\n",
    "        if list2[j] in tempList:\n",
    "            tags1[i,j]=1\n",
    "            \n",
    "\n",
    "            \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "columnsToEncode = list(train_features.select_dtypes(include=['category','object']))\n",
    "le = LabelEncoder()\n",
    "for feature in columnsToEncode:\n",
    "    try:\n",
    "        train_features[feature] = le.fit_transform(train_features[feature].astype(str))\n",
    "    except:\n",
    "        print('Error encoding ' + feature)\n",
    "        \n",
    "columnsToEncode = list(train_labels.select_dtypes(include=['category','object']))\n",
    "le = LabelEncoder()\n",
    "for feature in columnsToEncode:\n",
    "    try:\n",
    "        train_labels[feature] = le.fit_transform(train_labels[feature].astype(str))\n",
    "    except:\n",
    "        print('Error encoding ' + feature)\n",
    "        \n",
    "temp_features = np.array(train_features)\n",
    "temp_features = np.delete(temp_features,4,axis = 1)\n",
    "for i in range(5240):\n",
    "    temp_features[i,3] = temp_features[i,3]+1915\n",
    "#print(temp_features[:,3])\n",
    "new_features = np.append(tags1,temp_features[:,4:],axis = 1)\n",
    "\n",
    "train_labels1 = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_features = pd.read_csv(open(\"valid_features.tsv\"),sep='\\t')\n",
    "valid_labels = pd.read_csv(open(\"valid_labels.tsv\"),sep='\\t')\n",
    "\n",
    "valid_tags = np.array(valid_features['tag'],dtype = np.str)\n",
    "\n",
    "valid_tags1 = np.zeros((299,200))\n",
    "for i in range(299):\n",
    "    for j in range(200):\n",
    "        tempList = valid_tags[i].split(',')\n",
    "        if list2[j] in tempList:\n",
    "            valid_tags1[i,j]=1\n",
    "            \n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "columnsToEncode = list(valid_features.select_dtypes(include=['category','object']))\n",
    "le = LabelEncoder()\n",
    "for feature in columnsToEncode:\n",
    "    try:\n",
    "        valid_features[feature] = le.fit_transform(valid_features[feature].astype(str))\n",
    "    except:\n",
    "        print('Error encoding ' + feature)\n",
    "        \n",
    "\n",
    "\n",
    "columnsToEncode = list(valid_labels.select_dtypes(include=['category','object']))\n",
    "le = LabelEncoder()\n",
    "for feature in columnsToEncode:\n",
    "    try:\n",
    "        valid_labels[feature] = le.fit_transform(valid_labels[feature].astype(str))\n",
    "    except:\n",
    "        print('Error encoding ' + feature)\n",
    "        \n",
    "temp_valid_features = np.array(valid_features)\n",
    "temp_valid_features = np.delete(temp_valid_features,4,axis = 1)\n",
    "for i in range(299):\n",
    "    temp_valid_features[i,3] = temp_valid_features[i,3]+1915\n",
    "\n",
    "new_valid_features = np.append(valid_tags1,temp_valid_features[:,4:],axis = 1)\n",
    "\n",
    "valid_labels1 = np.array(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = pd.read_csv(open(\"test_features.tsv\"),sep='\\t')\n",
    "\n",
    "test_tags = np.array(test_features['tag'],dtype = np.str)\n",
    "\n",
    "test_tags1 = np.zeros((235,200))\n",
    "for i in range(235):\n",
    "    for j in range(200):\n",
    "        tempList = test_tags[i].split(',')\n",
    "        if list2[j] in tempList:\n",
    "            test_tags1[i,j]=1\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "columnsToEncode = list(test_features.select_dtypes(include=['category','object']))\n",
    "le = LabelEncoder()\n",
    "for feature in columnsToEncode:\n",
    "    try:\n",
    "        test_features[feature] = le.fit_transform(test_features[feature].astype(str))\n",
    "    except:\n",
    "        print('Error encoding ' + feature)\n",
    "        \n",
    "        \n",
    "temp_test_features = np.array(test_features)\n",
    "temp_test_features = np.delete(temp_test_features,4,axis = 1)\n",
    "for i in range(235):\n",
    "    temp_test_features[i,3] = temp_test_features[i,3]+1915\n",
    "\n",
    "new_test_features = np.append(test_tags1,temp_test_features[:,4:],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = np.zeros((5240,18))\n",
    "for i in range(5240):\n",
    "    for j in range(18):\n",
    "        if train_labels1[i,1]==j:\n",
    "            new_labels[i,j]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_labels111 = np.zeros((299,18))\n",
    "for i in range(299):\n",
    "    for j in range(18):\n",
    "        if valid_labels1[i,1]==j:\n",
    "            valid_labels111[i,j]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "N = 100 \n",
    "D = 327 \n",
    "num_label = 18 \n",
    "num_data = 5240\n",
    "hidden_size_1 = 40\n",
    "hidden_size_2 = 40\n",
    "hidden_size_3 = 80\n",
    "\n",
    "beta = 0.001 \n",
    "learning_rate = 0.001 \n",
    "\n",
    "#labels = (np.arange(num_label) == y[:,None]).astype(np.float32)\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.constant(new_features.astype(np.float32))\n",
    "    tf_labels = tf.constant(new_labels)\n",
    "    \n",
    "    # hidden layer 1\n",
    "    hidden_layer_weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([D, hidden_size_1], stddev=math.sqrt(2.0/num_data)))\n",
    "    hidden_layer_bias_1 = tf.Variable(tf.zeros([hidden_size_1]))\n",
    "    \n",
    "    # hidden layer 2\n",
    "    hidden_layer_weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_size_1, hidden_size_2], stddev=math.sqrt(2.0/hidden_size_1)))\n",
    "    hidden_layer_bias_2 = tf.Variable(tf.zeros([hidden_size_2]))\n",
    "    '''    \n",
    "    # hidden layer 2\n",
    "    hidden_layer_weights_3 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_size_2, hidden_size_3], stddev=math.sqrt(2.0/hidden_size_2)))\n",
    "    hidden_layer_bias_3 = tf.Variable(tf.zeros([hidden_size_3]))\n",
    "   \n",
    "    '''\n",
    "    # output layer\n",
    "    out_weights = tf.Variable(\n",
    "    tf.truncated_normal([hidden_size_2, num_label], stddev=math.sqrt(2.0/hidden_size_2)))\n",
    "    out_bias = tf.Variable(tf.zeros([num_label]))\n",
    "    \n",
    "\n",
    "    z1 = tf.matmul(x, hidden_layer_weights_1) + hidden_layer_bias_1\n",
    "    h1 = tf.nn.relu(z1)\n",
    "    \n",
    "    z2 = tf.matmul(h1, hidden_layer_weights_2) + hidden_layer_bias_2\n",
    "    h2 = tf.nn.relu(z2)\n",
    "    \"\"\"\n",
    "    z3 = tf.matmul(h2, hidden_layer_weights_3) + hidden_layer_bias_3\n",
    "    h3 = tf.nn.relu(z3)\n",
    "    \"\"\"\n",
    "    logits = tf.matmul(h2, out_weights) + out_bias\n",
    "    \n",
    "    # L2 regularization\n",
    "    regularization = tf.nn.l2_loss(hidden_layer_weights_1) + tf.nn.l2_loss(hidden_layer_weights_2)  + tf.nn.l2_loss(out_weights)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_labels, logits=logits) + beta * regularization) \n",
    "    #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_labels, logits=logits))\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    weights = [hidden_layer_weights_1, hidden_layer_bias_1, hidden_layer_weights_2, hidden_layer_bias_2, out_weights, out_bias]\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 6.417058\n",
      "Training accuracy: 4.7%\n",
      "Loss at step 200: 2.153193\n",
      "Training accuracy: 36.5%\n",
      "Loss at step 400: 1.827898\n",
      "Training accuracy: 45.4%\n",
      "Loss at step 600: 1.716215\n",
      "Training accuracy: 49.2%\n",
      "Loss at step 800: 1.656546\n",
      "Training accuracy: 51.4%\n",
      "Loss at step 1000: 1.610652\n",
      "Training accuracy: 52.9%\n",
      "Loss at step 1200: 1.580900\n",
      "Training accuracy: 54.0%\n",
      "Loss at step 1400: 1.545078\n",
      "Training accuracy: 55.3%\n",
      "Loss at step 1600: 1.536815\n",
      "Training accuracy: 55.6%\n",
      "Loss at step 1800: 1.511133\n",
      "Training accuracy: 56.9%\n",
      "Loss at step 2000: 1.490168\n",
      "Training accuracy: 57.0%\n",
      "Loss at step 2200: 1.472742\n",
      "Training accuracy: 58.0%\n",
      "Loss at step 2400: 1.459695\n",
      "Training accuracy: 59.0%\n",
      "Loss at step 2600: 1.440844\n",
      "Training accuracy: 59.8%\n",
      "Loss at step 2800: 1.423917\n",
      "Training accuracy: 61.2%\n",
      "Validation accuracy: 39.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3000\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    #print(predictions.shape[0])\n",
    "    #print(np.argmax(labels, axis=1))\n",
    "    return (100.0 * np.sum(np.argmax(predictions, axis=1) == np.argmax(labels, axis=1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "          \n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        _, l, predictions = session.run([train_op, loss, train_prediction])\n",
    "    \n",
    "        if (step % 200 == 0):\n",
    "            #print(predictions)\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Training accuracy: %.1f%%' % accuracy(\n",
    "                predictions, new_labels))\n",
    "    w1, b1, w2, b2, w3, b3 = weights\n",
    "    #print(w1)\n",
    "    #print(w1.eval())\n",
    "    \n",
    "    #Z = np.dot(np.dot(np.dot(new_valid_features, w1.eval()) + b1.eval(), w2.eval()) + b2.eval(), w3.eval()) + b3.eval()\n",
    "    Z = np.dot(relu(np.dot(relu(np.dot(new_valid_features, w1.eval()) + b1.eval()), w2.eval()) + b2.eval()), w3.eval()) + b3.eval()\n",
    "    pred = np.dot(relu(np.dot(relu(np.dot(new_test_features, w1.eval()) + b1.eval()), w2.eval()) + b2.eval()), w3.eval()) + b3.eval()\n",
    "    print('Validation accuracy: %.1f%%' % accuracy(Z, valid_labels111))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network precision(macro):\n",
      "37.27%\n",
      "Neural Network recall(macro):\n",
      "30.41%\n",
      "Neural Network F-Score(macro):\n",
      "33.49%\n"
     ]
    }
   ],
   "source": [
    "Z1=np.argmax(Z, axis=1)\n",
    "\n",
    "P_R = np.zeros((18,4))\n",
    "actual = valid_labels1[:,-1]\n",
    "predicted = Z1.flatten()\n",
    "num_of_each_grade = np.zeros((18,1)).flatten()\n",
    "for i in range(18):\n",
    "    sum1 = 0\n",
    "    sum2 = 0\n",
    "    sum3 = 0\n",
    "    sum4 = 0\n",
    "    sum_of_instance = 0\n",
    "    for j in range(len(actual)):\n",
    "        if actual[j]==predicted[j]==(i):\n",
    "            sum1 = sum1+1\n",
    "        if actual[j]==(i)!=predicted[j]:\n",
    "            sum2 = sum2+1\n",
    "        if predicted[j]==(i)!=actual[j]:\n",
    "            sum3 = sum3+1\n",
    "        if predicted[j]!=(i) and actual[j]!=(i):\n",
    "            sum4 = sum4+1\n",
    "        if actual[j]==(i):\n",
    "            sum_of_instance = sum_of_instance+1\n",
    "    num_of_each_grade[i] = sum_of_instance\n",
    "    P_R[i,0] = sum1\n",
    "    P_R[i,1] = sum2\n",
    "    P_R[i,2] = sum3\n",
    "    P_R[i,3] = sum4\n",
    "\n",
    "closer_evaluation = np.zeros((18,3))\n",
    "for i in range(18):\n",
    "    if((P_R[i,0]+P_R[i,2])!=0):\n",
    "        closer_evaluation[i,0]=P_R[i,0]/(P_R[i,0]+P_R[i,2])\n",
    "    if((P_R[i,0]+P_R[i,1])!=0):\n",
    "        closer_evaluation[i,1]=P_R[i,0]/(P_R[i,0]+P_R[i,1])\n",
    "    P = closer_evaluation[i,0]\n",
    "    R = closer_evaluation[i,1]\n",
    "    if(P==0 and R==0):\n",
    "        closer_evaluation[i,2]=0\n",
    "    else:\n",
    "        closer_evaluation[i,2]= 2*P*R/(P+R)\n",
    "precision = 0\n",
    "for i in range(18):\n",
    "    precision = precision + closer_evaluation[i,0]\n",
    "precision = precision/18\n",
    "\n",
    "recall = 0\n",
    "for i in range(18):\n",
    "    recall = recall + closer_evaluation[i,1]\n",
    "recall = recall/18\n",
    "\n",
    "fscore = 2*precision*recall/(precision+recall)\n",
    "\n",
    "print(\"Neural Network precision(macro):\\n\"+'%.2f%%'%(precision*100))\n",
    "print(\"Neural Network recall(macro):\\n\"+'%.2f%%'%(recall*100))\n",
    "print(\"Neural Network F-Score(macro):\\n\"+'%.2f%%'%(fscore*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['movieId', 'genres'],\n",
       "       ['91511', 'Comedy'],\n",
       "       ['91535', 'Fantasy'],\n",
       "       ['91542', 'Crime'],\n",
       "       ['91797', 'Drama'],\n",
       "       ['91826', 'Drama'],\n",
       "       ['91886', 'Drama'],\n",
       "       ['91888', 'Animation'],\n",
       "       ['91911', 'Drama'],\n",
       "       ['91976', 'Romance'],\n",
       "       ['92198', 'Thriller'],\n",
       "       ['92206', 'Drama'],\n",
       "       ['92210', 'Fantasy'],\n",
       "       ['92224', 'Drama'],\n",
       "       ['92234', 'War'],\n",
       "       ['92243', 'War'],\n",
       "       ['92352', 'Drama'],\n",
       "       ['92391', 'Horror'],\n",
       "       ['92475', 'War'],\n",
       "       ['92507', 'Thriller'],\n",
       "       ['92652', 'Documentary'],\n",
       "       ['92768', 'Drama'],\n",
       "       ['93132', 'Children'],\n",
       "       ['93181', 'Documentary'],\n",
       "       ['93183', 'Drama'],\n",
       "       ['93196', 'Comedy'],\n",
       "       ['93344', 'Sci_Fi'],\n",
       "       ['93367', 'Mystery'],\n",
       "       ['93475', 'Thriller'],\n",
       "       ['93496', 'Romance'],\n",
       "       ['93520', 'Romance'],\n",
       "       ['93563', 'Sci_Fi'],\n",
       "       ['93570', 'Drama'],\n",
       "       ['93831', 'Romance'],\n",
       "       ['93840', 'Horror'],\n",
       "       ['93865', 'Sci_Fi'],\n",
       "       ['93982', 'Thriller'],\n",
       "       ['94070', 'Romance'],\n",
       "       ['94271', 'Romance'],\n",
       "       ['94289', 'Drama'],\n",
       "       ['94365', 'Romance'],\n",
       "       ['94444', 'Comedy'],\n",
       "       ['94666', 'Romance'],\n",
       "       ['94727', 'Drama'],\n",
       "       ['94729', 'War'],\n",
       "       ['94790', 'Drama'],\n",
       "       ['94810', 'Sci_Fi'],\n",
       "       ['94933', 'Drama'],\n",
       "       ['94982', 'Drama'],\n",
       "       ['95214', 'Comedy'],\n",
       "       ['95352', 'Romance'],\n",
       "       ['95461', 'Romance'],\n",
       "       ['95543', 'Thriller'],\n",
       "       ['95558', 'Romance'],\n",
       "       ['95765', 'Romance'],\n",
       "       ['95839', 'Fantasy'],\n",
       "       ['95858', 'Animation'],\n",
       "       ['96034', 'Mystery'],\n",
       "       ['96195', 'Drama'],\n",
       "       ['96401', 'Drama'],\n",
       "       ['96419', 'Thriller'],\n",
       "       ['96421', 'Drama'],\n",
       "       ['96471', 'Crime'],\n",
       "       ['96588', 'Musical'],\n",
       "       ['96596', 'Sci_Fi'],\n",
       "       ['96634', 'Thriller'],\n",
       "       ['96700', 'Thriller'],\n",
       "       ['96751', 'Thriller'],\n",
       "       ['96815', 'Horror'],\n",
       "       ['96832', 'Fantasy'],\n",
       "       ['96870', 'Romance'],\n",
       "       ['96923', 'Thriller'],\n",
       "       ['97024', 'Thriller'],\n",
       "       ['97031', 'Horror'],\n",
       "       ['97070', 'Drama'],\n",
       "       ['97230', 'Documentary'],\n",
       "       ['97308', 'Drama'],\n",
       "       ['97324', 'Drama'],\n",
       "       ['97393', 'Drama'],\n",
       "       ['97460', 'Romance'],\n",
       "       ['97673', 'Drama'],\n",
       "       ['97792', 'Thriller'],\n",
       "       ['97906', 'Comedy'],\n",
       "       ['97936', 'Romance'],\n",
       "       ['97957', 'Drama'],\n",
       "       ['97971', 'Drama'],\n",
       "       ['98296', 'Drama'],\n",
       "       ['98373', 'Romance'],\n",
       "       ['98807', 'Romance'],\n",
       "       ['99043', 'Romance'],\n",
       "       ['99296', 'Sci_Fi'],\n",
       "       ['99314', 'Romance'],\n",
       "       ['99317', 'Fantasy'],\n",
       "       ['99397', 'Sci_Fi'],\n",
       "       ['99448', 'Documentary'],\n",
       "       ['99487', 'Crime'],\n",
       "       ['99739', 'Thriller'],\n",
       "       ['99941', 'Romance'],\n",
       "       ['100244', 'Romance'],\n",
       "       ['100248', 'Fantasy'],\n",
       "       ['100302', 'Romance'],\n",
       "       ['100326', 'Thriller'],\n",
       "       ['100390', 'Comedy'],\n",
       "       ['100450', 'Fantasy'],\n",
       "       ['100556', 'Documentary'],\n",
       "       ['101016', 'Drama'],\n",
       "       ['101088', 'Mystery'],\n",
       "       ['101182', 'Fantasy'],\n",
       "       ['101264', 'Western'],\n",
       "       ['101283', 'Comedy'],\n",
       "       ['101362', 'Mystery'],\n",
       "       ['101763', 'Romance'],\n",
       "       ['101967', 'Drama'],\n",
       "       ['102088', 'War'],\n",
       "       ['102125', 'Sci_Fi'],\n",
       "       ['102176', 'Documentary'],\n",
       "       ['102194', 'Romance'],\n",
       "       ['102280', 'Drama'],\n",
       "       ['102497', 'Drama'],\n",
       "       ['102684', 'Horror'],\n",
       "       ['102716', 'Thriller'],\n",
       "       ['102991', 'Horror'],\n",
       "       ['103228', 'Sci_Fi'],\n",
       "       ['103366', 'Thriller'],\n",
       "       ['103481', 'Sci_Fi'],\n",
       "       ['103554', 'Thriller'],\n",
       "       ['103590', 'Sci_Fi'],\n",
       "       ['103624', 'Romance'],\n",
       "       ['103801', 'Romance'],\n",
       "       ['103810', 'Thriller'],\n",
       "       ['103834', 'Sci_Fi'],\n",
       "       ['103996', 'Drama'],\n",
       "       ['104011', 'Romance'],\n",
       "       ['104017', 'Romance'],\n",
       "       ['104041', 'Sci_Fi'],\n",
       "       ['104069', 'Thriller'],\n",
       "       ['104226', 'Drama'],\n",
       "       ['104241', 'Thriller'],\n",
       "       ['104245', 'Children'],\n",
       "       ['104247', 'Comedy'],\n",
       "       ['104303', 'Drama'],\n",
       "       ['104627', 'Drama'],\n",
       "       ['104638', 'Thriller'],\n",
       "       ['104908', 'Horror'],\n",
       "       ['104944', 'Drama'],\n",
       "       ['104969', 'Romance'],\n",
       "       ['105135', 'Fantasy'],\n",
       "       ['105379', 'Sci_Fi'],\n",
       "       ['105382', 'Drama'],\n",
       "       ['105746', 'Comedy'],\n",
       "       ['105767', 'Documentary'],\n",
       "       ['105801', 'Drama'],\n",
       "       ['105805', 'Drama'],\n",
       "       ['106098', 'Documentary'],\n",
       "       ['106193', 'Drama'],\n",
       "       ['106210', 'Thriller'],\n",
       "       ['106240', 'Sci_Fi'],\n",
       "       ['106332', 'Drama'],\n",
       "       ['106616', 'Horror'],\n",
       "       ['106766', 'Crime'],\n",
       "       ['107042', 'Comedy'],\n",
       "       ['107050', 'Romance'],\n",
       "       ['107083', 'Romance'],\n",
       "       ['107117', 'Documentary'],\n",
       "       ['107143', 'Drama'],\n",
       "       ['107196', 'Documentary'],\n",
       "       ['107357', 'Drama'],\n",
       "       ['107501', 'Romance'],\n",
       "       ['107730', 'Drama'],\n",
       "       ['107737', 'Sci_Fi'],\n",
       "       ['107955', 'Thriller'],\n",
       "       ['107962', 'Thriller'],\n",
       "       ['108007', 'Romance'],\n",
       "       ['108190', 'Fantasy'],\n",
       "       ['108425', 'Thriller'],\n",
       "       ['108512', 'Comedy'],\n",
       "       ['108729', 'Mystery'],\n",
       "       ['108928', 'Thriller'],\n",
       "       ['109042', 'Fantasy'],\n",
       "       ['109191', 'Drama'],\n",
       "       ['109313', 'Drama'],\n",
       "       ['109317', 'Comedy'],\n",
       "       ['109533', 'Thriller'],\n",
       "       ['109793', 'Sci_Fi'],\n",
       "       ['109800', 'Documentary'],\n",
       "       ['109897', 'Adventure'],\n",
       "       ['110039', 'Fantasy'],\n",
       "       ['110273', 'War'],\n",
       "       ['110447', 'Comedy'],\n",
       "       ['110553', 'Sci_Fi'],\n",
       "       ['110599', 'Drama'],\n",
       "       ['110613', 'Drama'],\n",
       "       ['110669', 'Romance'],\n",
       "       ['110884', 'War'],\n",
       "       ['110899', 'Thriller'],\n",
       "       ['111233', 'Mystery'],\n",
       "       ['111283', 'Comedy'],\n",
       "       ['111403', 'Thriller'],\n",
       "       ['111617', 'Romance'],\n",
       "       ['111659', 'Fantasy'],\n",
       "       ['111680', 'Romance'],\n",
       "       ['112070', 'Romance'],\n",
       "       ['112093', 'Drama'],\n",
       "       ['112328', 'Comedy'],\n",
       "       ['112623', 'Sci_Fi'],\n",
       "       ['112640', 'Sci_Fi'],\n",
       "       ['112733', 'Documentary'],\n",
       "       ['113064', 'Comedy'],\n",
       "       ['113773', 'Documentary'],\n",
       "       ['113855', 'Documentary'],\n",
       "       ['114044', 'Thriller'],\n",
       "       ['114236', 'Comedy'],\n",
       "       ['114252', 'Romance'],\n",
       "       ['114762', 'Romance'],\n",
       "       ['114836', 'Fantasy'],\n",
       "       ['114852', 'Drama'],\n",
       "       ['115135', 'Drama'],\n",
       "       ['115770', 'Drama'],\n",
       "       ['115881', 'Sci_Fi'],\n",
       "       ['116797', 'War'],\n",
       "       ['116897', 'Drama'],\n",
       "       ['117887', 'Comedy'],\n",
       "       ['118696', 'Fantasy'],\n",
       "       ['119167', 'Fantasy'],\n",
       "       ['119575', 'Romance'],\n",
       "       ['119655', 'Fantasy'],\n",
       "       ['120104', 'Comedy'],\n",
       "       ['120110', 'Romance'],\n",
       "       ['120466', 'Sci_Fi'],\n",
       "       ['120625', 'Romance'],\n",
       "       ['120775', 'Romance'],\n",
       "       ['121342', 'Romance'],\n",
       "       ['125918', 'Comedy'],\n",
       "       ['126591', 'Fantasy'],\n",
       "       ['127993', 'Fantasy'],\n",
       "       ['128671', 'Drama']], dtype='<U32')"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1=np.argmax(pred, axis=1)\n",
    "\n",
    "#pred2 = np.ones((235,1), np.dtype = np.string)\n",
    "\n",
    "a = np.ones((235,1)) \n",
    "b = a.astype(np.str)\n",
    "for i in range(235):\n",
    "    if pred1[i]==0:\n",
    "        b[i,0]='Action'\n",
    "    if pred1[i]==1:\n",
    "        b[i,0]='Adventure'\n",
    "    if pred1[i]==2:\n",
    "        b[i,0]='Animation'\n",
    "    if pred1[i]==3:\n",
    "        b[i,0]='Children'\n",
    "    if pred1[i]==4:\n",
    "        b[i,0]='Comedy'\n",
    "    if pred1[i]==5:\n",
    "        b[i,0]='Crime'\n",
    "    if pred1[i]==6:\n",
    "        b[i,0]='Documentary'\n",
    "    if pred1[i]==7:\n",
    "        b[i,0]='Drama'\n",
    "    if pred1[i]==8:\n",
    "        b[i,0]='Fantasy'\n",
    "    if pred1[i]==9:\n",
    "        b[i,0]='Film_Noir'\n",
    "    if pred1[i]==10:\n",
    "        b[i,0]='Horror'\n",
    "    if pred1[i]==11:\n",
    "        b[i,0]='Musical'\n",
    "    if pred1[i]==12:\n",
    "        b[i,0]='Mystery'\n",
    "    if pred1[i]==13:\n",
    "        b[i,0]='Romance'\n",
    "    if pred1[i]==14:\n",
    "        b[i,0]='Sci_Fi'\n",
    "    if pred1[i]==15:\n",
    "        b[i,0]='Thriller'\n",
    "    if pred1[i]==16:\n",
    "        b[i,0]='War'\n",
    "    if pred1[i]==17:\n",
    "        b[i,0]='Western'\n",
    "movieid = np.array(temp_test_features[:,:1],dtype=np.int)\n",
    "no_head_final_result = np.append(movieid,b,axis = 1)\n",
    "head = np.array([['movieId','genres']])\n",
    "final_result=np.append(head,no_head_final_result,axis = 0)\n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.savetxt(\"final_result.csv\", final_result, fmt='%s',delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
